{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2 data extraction\n",
    "This notebook contains code to extract the audio data from the NSX file, generate an automatic\n",
    "transcript, calculate the spike rates from recorded spike events\n",
    "and align the spikerates with the transcribed sentences. The goal is to create a dataset consisting of feature (spike rates) / target (sentences) pairs that can be used to train a decoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "from math import ceil\n",
    "\n",
    "import brpylib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "from tqdm import tqdm\n",
    "\n",
    "from thesis_project.settings import DATA_DIR, EXPERIMENT2_DIR\n",
    "from thesis_project.data_loading import construct_spikerates_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural and audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hub1-20240708-141522-001.nev opened\n",
      "\n",
      "NSP-20240708-141522-001.ns6 opened\n"
     ]
    }
   ],
   "source": [
    "nev_file1 = brpylib.NevFile(EXPERIMENT2_DIR + \"/Experiment/20240708-141522/Hub1-20240708-141522-001.nev\") # contains the audio recording\n",
    "nsx_file = brpylib.NsxFile(EXPERIMENT2_DIR+ \"/Experiment/20240708-141522/NSP-20240708-141522-001.ns6\") # contains the spike events\n",
    "spike_events = nev_file1.getdata(wave_read=True)['spike_events']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = nsx_file.basic_header['SampleResolution']\n",
    "sorted_timestamps = spike_events['TimeStamps']\n",
    "sorted_timestamps.sort() # spike event timestamps in nanoseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data = nsx_file.getdata()['data'][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create audio transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_PATH = f\"{DATA_DIR}/audio/recording.wav\"\n",
    "WHISPER_RESULT_PATH = f\"{DATA_DIR}/audio/whisper_output.pkl\"\n",
    "WHISPER_RESULT_SENTENCES_PATH = f\"{DATA_DIR}/audio/whisper_result_sentences.pkl\"\n",
    "\n",
    "# save audio recording in a .wav file\n",
    "factor = 50000\n",
    "scaled = np.int16(audio_data / np.max(np.abs(audio_data)) * factor)\n",
    "write(AUDIO_PATH, sampling_rate, scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribe with whisperx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install whisperx\n",
    "\n",
    "import whisperx\n",
    "import gc\n",
    "\n",
    "device = \"cuda\"\n",
    "audio_file = \"clean_audio_end.wav\"\n",
    "language=\"de\"\n",
    "batch_size = 16 # reduce if low on GPU memory\n",
    "compute_type = \"float16\" # change to \"int8\" if low on GPU memory (may reduce accuracy)\n",
    "\n",
    "# transcribe with original whisper (batched)\n",
    "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
    "audio = whisperx.load_audio(\"recording.wav\")\n",
    "result = model.transcribe(audio, batch_size=batch_size)\n",
    "\n",
    "# align\n",
    "model_a, metadata = whisperx.load_align_model(language_code=language, device=device)\n",
    "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "\n",
    "# save file\n",
    "with open(WHISPER_RESULT_PATH, \"wb\") as file:\n",
    "  pickle.dump(result, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert timestamps\n",
    "Extract whisperx timestamps from the transcript (in seconds) and convert them to indices in the audio recording np array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WHISPER_RESULT_PATH, \"rb\") as file:\n",
    "  whisper_result = pickle.load(file)\n",
    "\n",
    "\n",
    "# multiply with sampling rate to get timestamps\n",
    "\n",
    "result_sentences = []\n",
    "\n",
    "for segment in whisper_result[\"segments\"]:\n",
    "    start = round(segment[\"start\"] * sampling_rate)\n",
    "    end = round(segment[\"end\"] * sampling_rate)\n",
    "    result_sentences.append((segment[\"text\"], (start, end)))\n",
    "\n",
    "# uncomment code to overwrite file\n",
    "# with open(WHISPER_RESULT_SENTENCES_PATH, \"wb\") as file:\n",
    "#     pickle.dump(result_sentences, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute spikerates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Load sentences\n",
    "# with open(WHISPER_RESULT_SENTENCES_PATH, \"rb\") as file:\n",
    "#     audio_segments = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load sentences\n",
    "with open(f\"{EXPERIMENT2_DIR}/whisper_sentences.pkl\", \"rb\") as file:\n",
    "    audio_segments = pickle.load(file)\n",
    "\n",
    "all_sentences = pd.read_csv(f\"{EXPERIMENT2_DIR}/labels.csv\")\n",
    "all_sentences = [(row[\"word\"], (row[\"start\"], row[\"end\"])) for _, row in all_sentences.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zum Beispiel, der Sprit für die Traktoren kostet bald mehr Geld.\n"
     ]
    }
   ],
   "source": [
    "# check if calculated indices correspond to the correct intervals in the audio recording\n",
    "sentence_idx = 75\n",
    "start_time, end_time = audio_segments[sentence_idx][1][0], audio_segments[sentence_idx][1][1]\n",
    "sentence = audio_segments[sentence_idx][0]\n",
    "sd.play(audio_data[start_time:end_time], samplerate=sampling_rate)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio / spike alignment\n",
    "The spike events' audio timestamps do not correspond to the time origin, so it is unclear how\n",
    "much time passes between the time origin and the first spike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.datetime(2024, 7, 8, 12, 15, 22, 950000), '2024-07-08 12:15:22')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spike file time origin\n",
    "nsx_file.basic_header['TimeOrigin'], datetime.strftime(nsx_file.basic_header['TimeOrigin'], '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.datetime(2024, 4, 14, 15, 23, 13, 72738), '2024-04-14 15:23:13')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first spike timestamp\n",
    "first_spike = datetime.fromtimestamp(sorted_timestamps[0] / 1e9)\n",
    "first_spike, datetime.strftime(first_spike, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am assuming that the first spike's timestamp corresponds to the beginning of the audio file,\n",
    "since they are closely aligned (roughly 1ms difference between the duration of the audio file\n",
    "and the timespan between the maximum and minimum spike timestamp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115.3453918995"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max timestamp - min timestamp (duration) in minutes\n",
    "(sorted_timestamps[-1] - sorted_timestamps[0]) / 1e9 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115.34549944444444"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio duration in minutes\n",
    "audio_data = nsx_file.getdata()\n",
    "len(audio_data['data'][0][0]) / sampling_rate / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00013333333333333334"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recording delay in minutes\n",
    "recording_delay = nsx_file.basic_header['TimeOrigin'] - nev_file1.basic_header['TimeOrigin'] # audio recording is started after neural recording\n",
    "recording_delay = recording_delay.microseconds * 1000 # convert to nanoseconds\n",
    "recording_delay / 1e9 / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute sentence spikerates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_timestamp, max_timestamp = sorted_timestamps[0], sorted_timestamps[-1]\n",
    "\n",
    "audio_segments = all_sentences\n",
    "\n",
    "audio_time_factor = 1e9 / sampling_rate # convert audio timestamp in seconds to nanoseconds\n",
    "buffer_before = 100 * 1e6 # optional buffer times in ms to account for delay in neural processing (~ 100ms)\n",
    "buffer_after = 100 * 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_spike_timestamp(start_timestamp: int, end_timestamp: int) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Convert start and end timestamp of an audio segment (e.g. a sentence or word) in seconds\n",
    "    to the corresponding index in the spike event array.\n",
    "    \"\"\"\n",
    "    return int(start_timestamp * audio_time_factor + buffer_before - recording_delay + min_timestamp), \\\n",
    "        int(end_timestamp * audio_time_factor + buffer_after - recording_delay + min_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script assigns each spike event to the sentence during which it occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1954676it [00:02, 772360.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# start and end timestamps of current audio audio segment\n",
    "# each audio segment corresponds to a sentence\n",
    "current_segment_start_timestamp, current_segment_end_timestamp = \\\n",
    "    audio_to_spike_timestamp(audio_segments[0][1][0], audio_segments[0][1][1])\n",
    "\n",
    "# index of current audio segment\n",
    "current_segment_index = 0\n",
    "\n",
    "# list lists of spike timestamp indices per segment index (i.e. index in the ´audio_segments list´)\n",
    "segment_timestamps = []\n",
    "\n",
    "# list of spike timestamps indices (i.e. indices in the ´sorted_timestamp´ list) corresponding to the current audio segment\n",
    "current_segment_timestamps = []\n",
    "\n",
    "# go over all spike timestamps and assign them to the corresponding audio segments, if applicable\n",
    "# assumes that spike timestamps and audio timestamps are in ascending order\n",
    "\n",
    "for i, event_timestamp in tqdm(enumerate(sorted_timestamps)):\n",
    "\n",
    "    if event_timestamp > current_segment_end_timestamp:\n",
    "        # increment the segment index if the current spike happens after the segment\n",
    "\n",
    "        while event_timestamp > current_segment_end_timestamp:\n",
    "            # skip all audio segments where no spikes occur\n",
    "\n",
    "            segment_timestamps.append(current_segment_timestamps)\n",
    "            current_segment_index += 1\n",
    "        \n",
    "\n",
    "            if current_segment_index >= len(audio_segments):\n",
    "                # stop when the last audio segment has been added\n",
    "                break\n",
    "\n",
    "            current_segment_start_timestamp, current_segment_end_timestamp = \\\n",
    "                audio_to_spike_timestamp(audio_segments[current_segment_index][1][0], audio_segments[current_segment_index][1][1])\n",
    "\n",
    "            # create empty spike timestamp array for current audio segment\n",
    "            current_segment_timestamps = []\n",
    "\n",
    "\n",
    "    if current_segment_index >= len(audio_segments):\n",
    "        # stop when the last audio segment has been added\n",
    "        break\n",
    "\n",
    "    if event_timestamp < current_segment_start_timestamp:\n",
    "        # spike has occurred between audio segments\n",
    "        continue\n",
    "\n",
    "    # append the current spike index to the current segment's timestamp list\n",
    "    current_segment_timestamps.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_audio_duration = 0 # in nanoseconds\n",
    "min_audio_timestamps = []\n",
    "\n",
    "# audio recording durations in ms\n",
    "for _, (start_time, end_time) in audio_segments:\n",
    "    duration = (end_time - start_time) * audio_time_factor\n",
    "    min_audio_timestamps.append(start_time)\n",
    "\n",
    "    if duration > max_audio_duration:\n",
    "        max_audio_duration = duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Maximum duration of a sentence in seconds:', 22.455)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Maximum duration of a sentence in seconds:\", max_audio_duration / 1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the spike rate timeseries for each sentence by binning the spike events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "650it [00:02, 318.90it/s]\n"
     ]
    }
   ],
   "source": [
    "n_channels = 256\n",
    "bin_size = 100 * 1e6 # small bin size in ms because of short sentence duration\n",
    "n_bins = ceil(max_audio_duration / bin_size) + 1\n",
    "\n",
    "spikerates = np.zeros((len(audio_segments), n_bins, n_channels))\n",
    "\n",
    "for i, (segment_idx, audio_segment) in tqdm(enumerate(zip(segment_timestamps, audio_segments))):\n",
    "\n",
    "    min_audio_timestamp = audio_segment[1][0] * audio_time_factor\n",
    "\n",
    "    for j in segment_idx:\n",
    "\n",
    "        timestamp = spike_events['TimeStamps'][j] - spike_events['TimeStamps'][0]\n",
    "        channel = spike_events['Channel'][j]\n",
    "\n",
    "        idx = int((timestamp - min_audio_timestamp) / bin_size)\n",
    "        spikerates[i][idx][channel - 1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(650, 226, 256)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spikerates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique spikerate timeseries: 650 of 650\n"
     ]
    }
   ],
   "source": [
    "# Sanity check:\n",
    "# Check the number of unique spikerate timeseries\n",
    "\n",
    "n_uniques = list(zip(*np.unique(spikerates, return_counts = True, axis = 0)))\n",
    "print(f\"Number of unique spikerate timeseries: {len(n_uniques)} of {len(spikerates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save spikerates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = nsx_file.basic_header['TimeOrigin'].strftime(\"%Y%m%d\")\n",
    "\n",
    "spikerates_filename = construct_spikerates_filename(session_id=session_id,\n",
    "                                                    path=f\"{EXPERIMENT2_DIR}/binned_spikerates\",\n",
    "                                                    bin_size=int(bin_size / 1e6),\n",
    "                                                    experiment=\"experiment2\")\n",
    "\n",
    "with open(file=spikerates_filename, mode=\"wb\") as file:\n",
    "    pickle.dump(spikerates, file)\n",
    "\n",
    "sentences = [audio_segment[0] for audio_segment in audio_segments]\n",
    "with open(file=f\"{EXPERIMENT2_DIR}/sentences_new.pkl\", mode=\"wb\") as file:\n",
    "    pickle.dump(sentences, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
